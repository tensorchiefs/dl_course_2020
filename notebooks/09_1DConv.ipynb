{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "09_1DConv_sol.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kcB3r4vJpatQ"
      },
      "source": [
        "## Demsonstration of 1-D and 1-D time dilitated convolutions\n",
        "\n",
        "in this notebook you will use 1D causal convolution to predict timeseries. You learn how 1D convolutions with causal padding work and see that dilated causal convolution are able capture long-range dependencies.\n",
        "\n",
        "**Dataset:** We will use an articfical dataset with some long-range dependencies. We generate 1000 timeseries with the length of 128 timesteps that all follow the same pattern: a fast changing sine wave where the amplitude is modulated by an other sine wave with lower frequency. To make it a bit more challenging, we add some noise at each timestep of the waves. Our goal is to first predict the next 10 timesteps and then we want to predict for even longer timesteps in the future.\n",
        "\n",
        "**Content:**\n",
        "* creat articfical dataset with some long-range dependencies\n",
        "* visualize a samples timeseries\n",
        "* use keras to train a CNN with 1D causal convolutions\n",
        "* predict the next 10 timesteps of the timeseries \n",
        "* predict the next 80 timesteps of the timeseries \n",
        "* use keras to train a CNN with 1D dilitated causal convolutions (which are abe to deal with long-range dependencies)\n",
        "* predict the next 80 timesteps of the timeseries with 1D dilitated causal convolutions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "geSS6viBpatT",
        "colab": {}
      },
      "source": [
        "# load required libraries:\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('default')\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Convolution2D, MaxPooling2D, Flatten , Activation, Lambda, Convolution1D\n",
        "from tensorflow.keras.utils import to_categorical \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "82ZRKcWopate"
      },
      "source": [
        "### Simulate some data\n",
        "\n",
        "In the next cell we generate train and validation data. We multiply a fast sine wave with a slower sine wave and add a bit random noise. The goal is to learn from the past of time series and predict the next 10 steps and later even more than \"only\"  10 steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OlA9Vabapatf",
        "colab": {}
      },
      "source": [
        "np.random.seed(1) # Fixing the seed, so data generation is always the same\n",
        "seq_length = 128  # Sequence length used for training\n",
        "look_ahead =  10  # The number of data points the model should predict \n",
        "\n",
        "\n",
        "def gen_data(size=1000, noise=0.1,seq_length=128,look_ahead=10): # We create 1000 observations of the process\n",
        "  s = seq_length + look_ahead\n",
        "  d = np.zeros((size, s,1))\n",
        "  for i in range(size):\n",
        "    start = np.random.uniform(0, 2*np.pi) # Random start point\n",
        "    d[i,:,0] = np.sin(start + np.linspace(0, 20*np.pi, s)) * np.sin(start + np.linspace(0, np.pi, s)) + np.random.normal(0,noise,s)\n",
        "  return d[:,0:seq_length], d[:,seq_length:s]\n",
        "\n",
        "\n",
        "X,Y = gen_data()\n",
        "for i in range(1):\n",
        "  plt.figure(num=None, figsize=(13,5))  \n",
        "  plt.plot(range(0, seq_length),X[i,:,0],'b-')\n",
        "  plt.plot(range(seq_length, seq_length + look_ahead),Y[i,:,0],'b-',color='orange')\n",
        "\n",
        "plt.show()\n",
        "print('The training data X is the blue line and we want to forecast the next 10 steps Y, the orange line.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zad2itQSpatl",
        "colab": {}
      },
      "source": [
        "# process:fast sine wave * slow sine wave + random noise\n",
        "start = np.random.uniform(0, 2*np.pi) # Random start point\n",
        "s=128+10\n",
        "plt.figure(figsize=(13,5))  \n",
        "plt.plot(np.sin(start + np.linspace(0, 20*np.pi, s)))\n",
        "plt.show()\n",
        "plt.figure(figsize=(13,5))  \n",
        "plt.plot(np.sin(start + np.linspace(0, np.pi, s)))\n",
        "plt.show()\n",
        "plt.figure(figsize=(13,5))  \n",
        "plt.plot(np.sin(start + np.linspace(0, 20*np.pi, s))*np.sin(start + np.linspace(0, np.pi, s)))\n",
        "plt.show()\n",
        "plt.figure(figsize=(13,5))  \n",
        "plt.plot(np.sin(start + np.linspace(0, 20*np.pi, s))*np.sin(start + np.linspace(0, np.pi, s))+np.random.normal(0,0.1,s))\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zWlpYlGrpatr",
        "colab": {}
      },
      "source": [
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YbHIUaw3paty"
      },
      "source": [
        "### 1D Convolution without dilation rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M8xZadZfp_z_"
      },
      "source": [
        "Here we define a Neural network with 1D convolutions and \"causal\" padding, in a later step we will also use a dilation rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eicNaym0patz",
        "colab": {}
      },
      "source": [
        "def slice(x, slice_length):\n",
        "    return x[:,-slice_length:,:]\n",
        "  \n",
        "model_1Dconv = Sequential()\n",
        "ks = 5\n",
        "model_1Dconv.add(Convolution1D(filters=32, kernel_size=ks, padding='causal', input_shape=(128, 1)))\n",
        "model_1Dconv.add(Convolution1D(filters=32, kernel_size=ks, padding='causal'))\n",
        "model_1Dconv.add(Convolution1D(filters=32, kernel_size=ks, padding='causal'))\n",
        "model_1Dconv.add(Convolution1D(filters=32, kernel_size=ks, padding='causal'))\n",
        "model_1Dconv.add(Dense(1))\n",
        "model_1Dconv.add(Lambda(slice, arguments={'slice_length':look_ahead}))\n",
        "\n",
        "model_1Dconv.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model_1Dconv.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q0yAb86Npat6",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "history = model_1Dconv.fit(X[0:800], Y[0:800],\n",
        "                    epochs=50,\n",
        "                    batch_size=128,\n",
        "                    validation_data=(X[800:1000],Y[800:1000]),\n",
        "                    verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5pjw6R4XpauA",
        "colab": {}
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Absolute Error Loss')\n",
        "plt.title('Loss Over Time')\n",
        "plt.legend(['Train','Valid'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_cR-davRpauF"
      },
      "source": [
        "Now we want to use the trained model to predict for the next 10 steps, remember that is what the model was trained on.  \n",
        "In addition,  we want to predict for longer than just 10 steps, we will  just predict the next 10 steps and take the predictions as new \"true\" observations and feed these values into the model, when we do that we can predict for any length we want. In the next cell we will predict the next 10 and 80 steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l3jEUp5FpauG",
        "colab": {}
      },
      "source": [
        "### predict for 10 steps\n",
        "i=950\n",
        "steps = 1\n",
        "pred = np.empty(0)\n",
        "data = X[i:i+1]\n",
        "model = model_1Dconv\n",
        "\n",
        "for j in range(0,steps):\n",
        "  res = model.predict(data)[0]\n",
        "  res = res.reshape(10)\n",
        "  data=(np.concatenate((data[0:1].reshape(128),res)))\n",
        "  data=data[-128:]\n",
        "  data=data.reshape((1,128,1))\n",
        "  #data=data.reshape(1,128+(j)*10,1)[-128:]\n",
        "  pred=np.append(pred,res)\n",
        "data=X[i:i+1]  \n",
        "plt.figure(num=None, figsize=(13,5))\n",
        "plt.plot(range(0,128),np.concatenate((data[0:1].reshape(128),pred))[0:128],color='blue')\n",
        "plt.plot(range(128,128+steps*10),np.concatenate((data[0:1].reshape(128),pred))[128:128+steps*10],color='orange')\n",
        "plt.axvline(x=128)\n",
        "\n",
        "### predict for 80 steps\n",
        "i=950\n",
        "steps = 8\n",
        "pred = np.empty(0)\n",
        "data = X[i:i+1]\n",
        "model = model_1Dconv\n",
        "\n",
        "for j in range(0,steps):\n",
        "  res = model.predict(data)[0]\n",
        "  res = res.reshape(10)\n",
        "  data=(np.concatenate((data[0:1].reshape(128),res)))\n",
        "  data=data[-128:]\n",
        "  data=data.reshape((1,128,1))\n",
        "  #data=data.reshape(1,128+(j)*10,1)[-128:]\n",
        "  pred=np.append(pred,res)\n",
        "data=X[i:i+1]  \n",
        "plt.figure(num=None, figsize=(13,5))\n",
        "plt.plot(range(0,128),np.concatenate((data[0:1].reshape(128),pred))[0:128],color='blue')\n",
        "plt.plot(range(128,128+steps*10),np.concatenate((data[0:1].reshape(128),pred))[128:128+steps*10],color='orange')\n",
        "plt.axvline(x=128)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gii40ZQqs-DJ"
      },
      "source": [
        "### 1D Convolution with dilation rate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KG-10GeCsi_s"
      },
      "source": [
        "Here we define a Neural network with 1D convolutions and \"causal\" padding, this time with dilation rate, so we are able to look back longer in time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BJb7Akn8pauM",
        "colab": {}
      },
      "source": [
        "\n",
        "model_1Dconv_w_d = Sequential()\n",
        "ks = 5\n",
        "##################################################\n",
        "###### your code here#############################\n",
        "\n",
        "\n",
        "\n",
        "###### end of your code ##########################\n",
        "##################################################\n",
        "\n",
        "model_1Dconv_w_d.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model_1Dconv_w_d.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TiZRwd73pauQ",
        "colab": {}
      },
      "source": [
        "history = model_1Dconv_w_d.fit(X[0:800], Y[0:800],\n",
        "                    epochs=50,\n",
        "                    batch_size=128,\n",
        "                    validation_data=(X[800:1000],Y[800:1000]),\n",
        "                    verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hUmeQM33paua",
        "colab": {}
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Absolute Error Loss')\n",
        "plt.title('Loss Over Time')\n",
        "plt.legend(['Train','Valid'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o7KT3mGXpauf",
        "colab": {}
      },
      "source": [
        "### predict for 10 steps\n",
        "i=950\n",
        "steps = 1\n",
        "pred = np.empty(0)\n",
        "data = X[i:i+1]\n",
        "model = model_1Dconv_w_d\n",
        "\n",
        "for j in range(0,steps):\n",
        "  res = model.predict(data)[0]\n",
        "  res = res.reshape(10)\n",
        "  data=(np.concatenate((data[0:1].reshape(128),res)))\n",
        "  data=data[-128:]\n",
        "  data=data.reshape((1,128,1))\n",
        "  #data=data.reshape(1,128+(j)*10,1)[-128:]\n",
        "  pred=np.append(pred,res)\n",
        "data=X[i:i+1]  \n",
        "plt.figure(num=None, figsize=(13,5))\n",
        "plt.plot(range(0,128),np.concatenate((data[0:1].reshape(128),pred))[0:128],color='blue')\n",
        "plt.plot(range(128,128+steps*10),np.concatenate((data[0:1].reshape(128),pred))[128:128+steps*10],color='orange')\n",
        "plt.axvline(x=128)\n",
        "\n",
        "### predict for 80 steps\n",
        "i=950\n",
        "steps = 8\n",
        "pred = np.empty(0)\n",
        "data = X[i:i+1]\n",
        "model = model_1Dconv_w_d\n",
        "\n",
        "for j in range(0,steps):\n",
        "  res = model.predict(data)[0]\n",
        "  res = res.reshape(10)\n",
        "  data=(np.concatenate((data[0:1].reshape(128),res)))\n",
        "  data=data[-128:]\n",
        "  data=data.reshape((1,128,1))\n",
        "  #data=data.reshape(1,128+(j)*10,1)[-128:]\n",
        "  pred=np.append(pred,res)\n",
        "data=X[i:i+1]  \n",
        "plt.figure(num=None, figsize=(13,5))\n",
        "plt.plot(range(0,128),np.concatenate((data[0:1].reshape(128),pred))[0:128],color='blue')\n",
        "plt.plot(range(128,128+steps*10),np.concatenate((data[0:1].reshape(128),pred))[128:128+steps*10],color='orange')\n",
        "plt.axvline(x=128)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exE4J-OSzJXB",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise:\n",
        "Compare the long time predictions of the 1D Convolutional netural network with and without dilation.  \n",
        "What do you observe and how do you exlain it."
      ]
    }
  ]
}